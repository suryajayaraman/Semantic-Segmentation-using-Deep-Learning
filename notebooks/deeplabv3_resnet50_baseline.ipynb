{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Welcome to the `deeplabv3` Workshop!\nIn this workshop, we'll learn the concept of how to use deeplabv3 model (involving Atrous convolutions, Atrous Spatial Pyramid Pooling) for Semantic Segmentation using Pytorch. We'll do the following tasks:\n\n- Dataset : Download and use BDD100k dataset\n- Network : Define deeplabv3 model using resnet50, Atrous convolutions, ASPP modules\n- Training : Train and validate model on the custom dataset\n- Evaluate : Evaluate the model on Test Data and visualize results","metadata":{"id":"LkfoFeGlYxHY","papermill":{"duration":0.010624,"end_time":"2022-06-26T08:49:42.237976","exception":false,"start_time":"2022-06-26T08:49:42.227352","status":"completed"},"tags":[]}},{"cell_type":"code","source":"try:\n    import segmentation_models_pytorch as smp\nexcept:\n    !pip install segmentation-models-pytorch\n    import segmentation_models_pytorch as smp","metadata":{"id":"-njz3v-jGxsi","outputId":"77838d12-01f5-4ab1-d1d8-8bbe08bf2cc1","papermill":{"duration":30.082697,"end_time":"2022-06-26T08:50:12.330123","exception":false,"start_time":"2022-06-26T08:49:42.247426","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:34:36.460551Z","iopub.execute_input":"2022-06-29T17:34:36.461127Z","iopub.status.idle":"2022-06-29T17:35:00.366410Z","shell.execute_reply.started":"2022-06-29T17:34:36.461006Z","shell.execute_reply":"2022-06-29T17:35:00.365351Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Basic Imports","metadata":{"id":"rKDStTzOQ_3R","papermill":{"duration":0.011439,"end_time":"2022-06-26T08:50:12.354610","exception":false,"start_time":"2022-06-26T08:50:12.343171","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# basic imports\nimport random\nimport numpy as np\n\n# DL library imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n# libraries for loading image, plotting \nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"id":"-73t7omkQ-y7","papermill":{"duration":0.194583,"end_time":"2022-06-26T08:50:12.561377","exception":false,"start_time":"2022-06-26T08:50:12.366794","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:00.368604Z","iopub.execute_input":"2022-06-29T17:35:00.369302Z","iopub.status.idle":"2022-06-29T17:35:00.527346Z","shell.execute_reply.started":"2022-06-29T17:35:00.369261Z","shell.execute_reply":"2022-06-29T17:35:00.526432Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 1. Dataset : Download and use BDD100k dataset","metadata":{"id":"1G6V9TIjsZB7","papermill":{"duration":0.011813,"end_time":"2022-06-26T08:50:12.585464","exception":false,"start_time":"2022-06-26T08:50:12.573651","status":"completed"},"tags":[]}},{"cell_type":"code","source":"ENVIRONMENT = 'kaggle'\n\nif ENVIRONMENT == 'kaggle':\n    dataset_path = '../input/image-segmentation'\n    output_path = '.'\n    \nelif ENVIRONMENT == 'colab':\n    import os\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    os.chdir(\"/content/drive/My Drive/thinkAutonomous/image_segmentation\")\n    dataset_path = 'dataset'\n    output_path = 'dataset'\n    \nelse:\n    raise NotImplementedError(\"Env can be kaggle or colab\")","metadata":{"papermill":{"duration":0.024334,"end_time":"2022-06-26T08:50:12.622090","exception":false,"start_time":"2022-06-26T08:50:12.597756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:00.530946Z","iopub.execute_input":"2022-06-29T17:35:00.531234Z","iopub.status.idle":"2022-06-29T17:35:00.537871Z","shell.execute_reply.started":"2022-06-29T17:35:00.531209Z","shell.execute_reply":"2022-06-29T17:35:00.536728Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"targetWidth = 320\ntargetHeight = 180\n\n# batch size for data loaders\nTRAIN_BATCH_SIZE = 8\nTEST_BATCH_SIZE  = 8\n\n# Hyperparameters\nN_EPOCHS = 10\nNUM_CLASSES = 3\nMAX_LR = 3e-4\nMODEL_NAME = 'deeplabv3_resnet50_baseline_61218'","metadata":{"id":"adLoblQrpzzy","outputId":"695af3d8-f020-4f37-9734-dcf9f0163229","papermill":{"duration":0.021882,"end_time":"2022-06-26T08:50:12.655679","exception":false,"start_time":"2022-06-26T08:50:12.633797","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:00.541278Z","iopub.execute_input":"2022-06-29T17:35:00.541702Z","iopub.status.idle":"2022-06-29T17:35:00.550132Z","shell.execute_reply.started":"2022-06-29T17:35:00.541642Z","shell.execute_reply":"2022-06-29T17:35:00.549070Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"images = np.load(f'{dataset_path}/image_{targetHeight}_{targetWidth}.npy')\nlabels = np.load(f'{dataset_path}/label_{targetHeight}_{targetWidth}.npy')\nprint(f\"RGB images shape = {images.shape}, Label images shape = {labels.shape}\")","metadata":{"papermill":{"duration":7.356877,"end_time":"2022-06-26T08:50:20.024259","exception":false,"start_time":"2022-06-26T08:50:12.667382","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:00.551769Z","iopub.execute_input":"2022-06-29T17:35:00.552082Z","iopub.status.idle":"2022-06-29T17:35:06.461959Z","shell.execute_reply.started":"2022-06-29T17:35:00.552057Z","shell.execute_reply":"2022-06-29T17:35:06.460831Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Torch Dataset definition","metadata":{"id":"qmFiD3i-0Crd","papermill":{"duration":0.011764,"end_time":"2022-06-26T08:50:20.048453","exception":false,"start_time":"2022-06-26T08:50:20.036689","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BDD100k_dataset(Dataset):\n    def __init__(self, images, labels, tf=None):\n        \"\"\"Dataset class for BDD100k_dataset drivable / segmentation data \"\"\"\n        self.images = images\n        self.labels = labels\n        self.tf = tf\n    \n    def __len__(self):\n        return self.images.shape[0]\n  \n    def __getitem__(self, index):\n        # read source image and convert to RGB, apply transform\n        rgb_image = self.images[index]\n        if self.tf is not None:\n            rgb_image = self.tf(rgb_image)\n\n        # read label image and convert to torch tensor\n        label_image  = torch.from_numpy(self.labels[index]).long()\n        return rgb_image, label_image  ","metadata":{"id":"ySpXTOvpmPk8","papermill":{"duration":0.026094,"end_time":"2022-06-26T08:50:20.086444","exception":false,"start_time":"2022-06-26T08:50:20.060350","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.463578Z","iopub.execute_input":"2022-06-29T17:35:06.463955Z","iopub.status.idle":"2022-06-29T17:35:06.472206Z","shell.execute_reply.started":"2022-06-29T17:35:06.463915Z","shell.execute_reply":"2022-06-29T17:35:06.470056Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Convert to torch tensor and normalize images using Imagenet values\npreprocess = transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=(0.485, 0.56, 0.406), std=(0.229, 0.224, 0.225))\n                ])\n\ndata = BDD100k_dataset(images, labels, tf=preprocess)","metadata":{"id":"r6XW6HajnzPq","papermill":{"duration":0.022318,"end_time":"2022-06-26T08:50:20.120884","exception":false,"start_time":"2022-06-26T08:50:20.098566","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.474077Z","iopub.execute_input":"2022-06-29T17:35:06.475521Z","iopub.status.idle":"2022-06-29T17:35:06.482224Z","shell.execute_reply.started":"2022-06-29T17:35:06.475483Z","shell.execute_reply":"2022-06-29T17:35:06.481334Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Splitting Training data into train and validation sets, creating Dataloaders","metadata":{"id":"rB9JLxjfoENJ","papermill":{"duration":0.012113,"end_time":"2022-06-26T08:50:20.144725","exception":false,"start_time":"2022-06-26T08:50:20.132612","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# split train data into train, validation and test sets\ntotal_count = len(data)\ntrain_count = int(0.7 * total_count) \nvalid_count = int(0.2 * total_count)\ntest_count = total_count - train_count - valid_count\ntrain_set, val_set, test_set = torch.utils.data.random_split(data, \n            (train_count, valid_count, test_count), generator=torch.Generator().manual_seed(1))\n\ntrain_dataloader = DataLoader(train_set, batch_size=TRAIN_BATCH_SIZE,drop_last=True)\nval_dataloader   = DataLoader(val_set, batch_size=TEST_BATCH_SIZE)\ntest_dataloader  = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)","metadata":{"id":"KKXj9c2qoC40","papermill":{"duration":0.026449,"end_time":"2022-06-26T08:50:20.182948","exception":false,"start_time":"2022-06-26T08:50:20.156499","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.484206Z","iopub.execute_input":"2022-06-29T17:35:06.484819Z","iopub.status.idle":"2022-06-29T17:35:06.494128Z","shell.execute_reply.started":"2022-06-29T17:35:06.484775Z","shell.execute_reply":"2022-06-29T17:35:06.493076Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Let's verify size of images from the dataset","metadata":{"id":"ku1BYOyXosMD","papermill":{"duration":0.01174,"end_time":"2022-06-26T08:50:20.206430","exception":false,"start_time":"2022-06-26T08:50:20.194690","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sample_image, sample_label = train_set[0]\nprint(f\"There are {len(train_set)} train images, {len(val_set)} validation images, {len(test_set)} test Images\")\nprint(f\"Input shape = {sample_image.shape}, output label shape = {sample_label.shape}\")","metadata":{"id":"yWrMEcQQota2","outputId":"088865c6-546f-4b68-a0b0-4a35a99d03f9","papermill":{"duration":0.030787,"end_time":"2022-06-26T08:50:20.249670","exception":false,"start_time":"2022-06-26T08:50:20.218883","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.495592Z","iopub.execute_input":"2022-06-29T17:35:06.495969Z","iopub.status.idle":"2022-06-29T17:35:06.509522Z","shell.execute_reply.started":"2022-06-29T17:35:06.495934Z","shell.execute_reply":"2022-06-29T17:35:06.508043Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Show Sample images from dataset","metadata":{"id":"MhMFn7YWprGr","papermill":{"duration":0.011704,"end_time":"2022-06-26T08:50:20.273389","exception":false,"start_time":"2022-06-26T08:50:20.261685","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# reference : https://github.com/bdd100k/bdd100k/blob/master/bdd100k/label/label.py\nfrom collections import namedtuple\nLabel = namedtuple( \"Label\", [ \"name\", \"train_id\", \"color\"])\ndrivables = [ \n             Label(\"direct\", 0, (219, 94, 86)),        # red\n             Label(\"alternative\", 1, (86, 211, 219)),  # cyan\n             Label(\"background\", 2, (0, 0, 0)),        # black          \n            ]\ntrain_id_to_color = [c.color for c in drivables if (c.train_id != -1 and c.train_id != 255)]\ntrain_id_to_color = np.array(train_id_to_color)","metadata":{"id":"jOQpj7kfq_tM","papermill":{"duration":0.024433,"end_time":"2022-06-26T08:50:20.309920","exception":false,"start_time":"2022-06-26T08:50:20.285487","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.513722Z","iopub.execute_input":"2022-06-29T17:35:06.514648Z","iopub.status.idle":"2022-06-29T17:35:06.521781Z","shell.execute_reply.started":"2022-06-29T17:35:06.514611Z","shell.execute_reply":"2022-06-29T17:35:06.520586Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# when using torch datasets we defined earlier, the output image\n# is normalized. So we're defining an inverse transformation to \n# transform to normal RGB format\ninverse_transform = transforms.Compose([\n        transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n    ])\n\nrgb_image, label = train_set[random.randint(0, len(train_set))]\nrgb_image = inverse_transform(rgb_image).permute(1, 2, 0).cpu().detach().numpy()\nlabel = label.cpu().detach().numpy()\n\n# plot sample image\nfig, axes = plt.subplots(1,2, figsize=(20,10))\naxes[0].imshow(rgb_image);\naxes[0].set_title(\"Image\");\naxes[0].axis('off');\naxes[1].imshow(train_id_to_color[label]);\naxes[1].set_title(\"Label\");\naxes[1].axis('off');","metadata":{"id":"UneBdP1LvZQr","outputId":"cf00c661-2725-4604-bc6c-ce4a72f11139","papermill":{"duration":0.391109,"end_time":"2022-06-26T08:50:20.713820","exception":false,"start_time":"2022-06-26T08:50:20.322711","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.525187Z","iopub.execute_input":"2022-06-29T17:35:06.525544Z","iopub.status.idle":"2022-06-29T17:35:06.842118Z","shell.execute_reply.started":"2022-06-29T17:35:06.525509Z","shell.execute_reply":"2022-06-29T17:35:06.841253Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Network : Define deeplabv3 model using resnet50, Atrous convolutions, ASPP modules","metadata":{"id":"zbOYP8UHc8s3","papermill":{"duration":0.016732,"end_time":"2022-06-26T08:50:20.748023","exception":false,"start_time":"2022-06-26T08:50:20.731291","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class aspp_conv(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation_rate):\n        super(aspp_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, dilation=dilation_rate, padding=dilation_rate, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU())\n        \n    def forward(self, x):\n        return self.conv(x)\n    \n    \nclass aspp_pool(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(aspp_pool, self).__init__()\n        self.pooling_module = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU())\n        \n    def forward(self, x):\n        input_size = x.shape[-2:]\n        x = self.pooling_module(x)\n        return F.interpolate(x, size=input_size, mode='bilinear', align_corners=False)\n    \n\nclass atrous_spatial_pyramid_pooling(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation_rates):\n        super(atrous_spatial_pyramid_pooling, self).__init__()\n\n        layers = nn.ModuleList([])\n        \n        # skip-connection, match the output channels\n        # using 1x1 convolutions\n        layers.append(nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1,bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        ))\n        \n        # spatial pyramid pooling wiht atrous convolutions\n        for rate in dilation_rates:\n            layers.append(aspp_conv(in_channels, out_channels, rate))\n            \n        # image pooling layer\n        layers.append(aspp_pool(in_channels, out_channels))\n        \n        # create Pytorch module list\n        self.layers = nn.ModuleList(layers)\n        \n        # 1x1 convolution to project concatenated output\n        # to desired number of channels\n        self.project = nn.Sequential(\n            nn.Conv2d(len(layers) * out_channels, out_channels, kernel_size=1,bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n    \n    def forward(self, x):\n        conv_outputs = []\n        for mod in self.layers:\n            mod_output = mod(x)\n            conv_outputs.append(mod_output)\n        \n        # concatenate output and reduce num_channels\n        output = self.project(torch.cat(conv_outputs, dim=1))\n        return output        \n    \n\nfrom torchvision.models import resnet50\n    \nclass deeplabv3(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(deeplabv3, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n                        \n        # backbone layers\n        backbone = resnet50(pretrained=True, replace_stride_with_dilation=[False, True, True])        \n        self.initial = nn.Sequential(*list(backbone.children())[:4])\n        self.layer1 = backbone.layer1\n        self.layer2 = backbone.layer2\n        self.layer3 = backbone.layer3\n        self.layer4 = backbone.layer4\n        \n        # ASPP modules\n        aspp_out_channels = 256\n        aspp_in_channels = int(backbone.fc.in_features)        \n        self.aspp_module = atrous_spatial_pyramid_pooling(aspp_in_channels, \n                       out_channels=aspp_out_channels, dilation_rates=[6, 12, 18])\n        \n        # classifier \n        self.cls = nn.Sequential(\n            nn.Conv2d(aspp_out_channels, aspp_out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(aspp_out_channels),\n            nn.ReLU(),\n            nn.Conv2d(aspp_out_channels, self.num_classes, kernel_size=1))\n                     \n                \n    def forward(self, x):\n        input_size = x.shape[-2:]\n        \n        # Pass input through Backbone layers\n        x = self.initial(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        # ASPP and classifier layers\n        x = self.aspp_module(x)\n        x = self.cls(x)\n        return F.interpolate(x, size=input_size, mode='bilinear')","metadata":{"papermill":{"duration":0.048324,"end_time":"2022-06-26T08:50:20.812891","exception":false,"start_time":"2022-06-26T08:50:20.764567","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.843205Z","iopub.execute_input":"2022-06-29T17:35:06.843508Z","iopub.status.idle":"2022-06-29T17:35:06.868450Z","shell.execute_reply.started":"2022-06-29T17:35:06.843479Z","shell.execute_reply":"2022-06-29T17:35:06.867404Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 3. Training : Train and validate model on the custom dataset","metadata":{"id":"HeI63CaFc8s6","papermill":{"duration":0.014939,"end_time":"2022-06-26T08:50:20.843048","exception":false,"start_time":"2022-06-26T08:50:20.828109","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\nBefore we train our model, we'll define some helper functions to calculate metric, plot training results etc","metadata":{"id":"4hBQ5Xmuc8s7","papermill":{"duration":0.015023,"end_time":"2022-06-26T08:50:20.873360","exception":false,"start_time":"2022-06-26T08:50:20.858337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Metric - meanIoU","metadata":{"id":"p2saoCUQc8s8","papermill":{"duration":0.014984,"end_time":"2022-06-26T08:50:20.903726","exception":false,"start_time":"2022-06-26T08:50:20.888742","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class meanIoU:\n    \"\"\" Class to find the mean IoU using confusion matrix approach \"\"\"    \n    def __init__(self, num_classes):\n        self.iou_metric = 0.0\n        self.num_classes = num_classes\n        # placeholder for confusion matrix on entire dataset\n        self.confusion_matrix = np.zeros((self.num_classes, self.num_classes))\n\n    def update(self, y_preds, labels):\n        \"\"\" Function finds the IoU for the input batch\n        and add batch metrics to overall metrics \"\"\"\n        predicted_labels = torch.argmax(y_preds, dim=1)\n        batch_confusion_matrix = self._fast_hist(labels.numpy().flatten(), predicted_labels.numpy().flatten())\n        self.confusion_matrix += batch_confusion_matrix\n    \n    def _fast_hist(self, label_true, label_pred):\n        \"\"\" Function to calculate confusion matrix on single batch \"\"\"\n        mask = (label_true >= 0) & (label_true < self.num_classes)\n        hist = np.bincount(\n            self.num_classes * label_true[mask].astype(int) + label_pred[mask],\n            minlength=self.num_classes ** 2,\n        ).reshape(self.num_classes, self.num_classes)\n        return hist\n\n    def compute(self):\n        \"\"\" Computes overall meanIoU metric from confusion matrix data \"\"\" \n        hist = self.confusion_matrix\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu)\n        return mean_iu\n\n    def reset(self):\n        self.iou_metric = 0.0\n        self.confusion_matrix = np.zeros((self.num_classes, self.num_classes))","metadata":{"id":"MRyngm4nc8s8","papermill":{"duration":0.031897,"end_time":"2022-06-26T08:50:20.950986","exception":false,"start_time":"2022-06-26T08:50:20.919089","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.870796Z","iopub.execute_input":"2022-06-29T17:35:06.871378Z","iopub.status.idle":"2022-06-29T17:35:06.882971Z","shell.execute_reply.started":"2022-06-29T17:35:06.871339Z","shell.execute_reply":"2022-06-29T17:35:06.881976Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Function to plot training curves","metadata":{"id":"tZBAn9xXc8s9","papermill":{"duration":0.014709,"end_time":"2022-06-26T08:50:20.980681","exception":false,"start_time":"2022-06-26T08:50:20.965972","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_training_results(df, model_name):\n    fig, ax1 = plt.subplots(figsize=(10,4))\n    ax1.set_ylabel('trainLoss', color='tab:red')\n    ax1.plot(df['epoch'].values, df['trainLoss'].values, color='tab:red')\n    ax1.tick_params(axis='y', labelcolor='tab:red')\n\n    ax2 = ax1.twinx()  \n    ax2.set_ylabel('validationLoss', color='tab:blue')\n    ax2.plot(df['epoch'].values, df['validationLoss'].values, color='tab:blue')\n    ax2.tick_params(axis='y', labelcolor='tab:blue')\n\n    plt.suptitle(f'{model_name} Training, Validation Curves')\n    plt.show()","metadata":{"id":"2Ft6-lRuc8s-","papermill":{"duration":0.027626,"end_time":"2022-06-26T08:50:21.023710","exception":false,"start_time":"2022-06-26T08:50:20.996084","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.884473Z","iopub.execute_input":"2022-06-29T17:35:06.885084Z","iopub.status.idle":"2022-06-29T17:35:06.895603Z","shell.execute_reply.started":"2022-06-29T17:35:06.885042Z","shell.execute_reply":"2022-06-29T17:35:06.894318Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Train validate function","metadata":{"id":"Ye0G3dMEc8s-","papermill":{"duration":0.01499,"end_time":"2022-06-26T08:50:21.053894","exception":false,"start_time":"2022-06-26T08:50:21.038904","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\ndef evaluate_model(model, dataloader, criterion, metric_class, device):\n    model.eval()\n    total_loss = 0.0\n    metric_object = metric_class(NUM_CLASSES)\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, total=len(dataloader)):\n            inputs = inputs.to(device)\n            labels = labels.to(device)                \n            y_preds = model(inputs)\n\n            # calculate loss\n            loss = criterion(y_preds, labels)\n            total_loss += loss.item()\n\n            # update batch metric information            \n            metric_object.update(y_preds.cpu().detach(), labels.cpu().detach())\n\n    evaluation_loss = total_loss / len(dataloader)\n    evaluation_metric = metric_object.compute()\n    return evaluation_loss, evaluation_metric","metadata":{"id":"rq5tJoYXc8s_","papermill":{"duration":0.028779,"end_time":"2022-06-26T08:50:21.098007","exception":false,"start_time":"2022-06-26T08:50:21.069228","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.898831Z","iopub.execute_input":"2022-06-29T17:35:06.899615Z","iopub.status.idle":"2022-06-29T17:35:06.909366Z","shell.execute_reply.started":"2022-06-29T17:35:06.899586Z","shell.execute_reply":"2022-06-29T17:35:06.908470Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train_validate_model(model, num_epochs, model_name, criterion, optimizer, \n                         device, dataloader_train, dataloader_valid, \n                         metric_class, metric_name, lr_scheduler = None):\n    # initialize placeholders for running values\n    results = []    \n    min_val_loss = np.Inf\n    len_train_loader = len(dataloader_train)\n\n    # move model to device\n    model.to(device)\n    \n    for epoch in range(num_epochs):\n        print(f\"Starting {epoch + 1} epoch ...\")\n        \n        # Training\n        model.train()\n        train_loss = 0.0\n        for inputs, labels in tqdm(dataloader_train, total=len_train_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device) \n\n            # Forward pass\n            y_preds = model(inputs)\n            loss = criterion(y_preds, labels)\n            train_loss += loss.item()\n              \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # adjust learning rate\n            if lr_scheduler is not None:\n                lr_scheduler.step()\n            \n        # compute per batch losses, metric value\n        train_loss = train_loss / len(dataloader_train)\n        validation_loss, validation_metric = evaluate_model(\n                        model, dataloader_valid, criterion, metric_class, device)\n\n        print(f'Epoch: {epoch+1}, trainLoss:{train_loss:6.5f}, validationLoss:{validation_loss:6.5f}, {metric_name}:{validation_metric: 4.2f}')\n        \n        # store results\n        results.append({'epoch': epoch, \n                        'trainLoss': train_loss, \n                        'validationLoss': validation_loss, \n                        f'{metric_name}': validation_metric})\n        \n        # if validation loss has decreased, save model and reset variable\n        if validation_loss <= min_val_loss:\n            min_val_loss = validation_loss\n            torch.save(model.state_dict(), f\"{output_path}/{model_name}.pt\")\n\n    # plot results\n    results = pd.DataFrame(results)\n    plot_training_results(results, model_name)\n    return results","metadata":{"id":"oSX6yKZQc8tA","papermill":{"duration":0.034135,"end_time":"2022-06-26T08:50:21.147099","exception":false,"start_time":"2022-06-26T08:50:21.112964","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.910884Z","iopub.execute_input":"2022-06-29T17:35:06.911270Z","iopub.status.idle":"2022-06-29T17:35:06.923636Z","shell.execute_reply.started":"2022-06-29T17:35:06.911235Z","shell.execute_reply":"2022-06-29T17:35:06.922530Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{"id":"cCnjwooT0Rde","papermill":{"duration":0.01544,"end_time":"2022-06-26T08:50:21.177545","exception":false,"start_time":"2022-06-26T08:50:21.162105","status":"completed"},"tags":[]}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n# reference : https://smp.readthedocs.io/en/latest/losses.html\ncriterion = smp.losses.DiceLoss('multiclass', classes=[0,1,2], log_loss = True, smooth=1.0)\n\n# create model, optimizer, lr_scheduler and pass to training function\nmodel = deeplabv3(in_channels=3, num_classes=NUM_CLASSES).to(device)\noptimizer = optim.Adam(model.parameters(), lr=MAX_LR)\nscheduler = OneCycleLR(optimizer, max_lr= MAX_LR, epochs = N_EPOCHS,steps_per_epoch = len(train_dataloader), \n                       pct_start=0.3, div_factor=10, anneal_strategy='cos')\n\n_ = train_validate_model(model, N_EPOCHS, MODEL_NAME, criterion, optimizer, \n                         device, train_dataloader, val_dataloader, meanIoU, 'meanIoU',\n                         lr_scheduler = scheduler)","metadata":{"id":"X2ubU3A2c8tB","outputId":"1c9d2251-1818-4467-9f74-ecd3731f8144","papermill":{"duration":1209.465044,"end_time":"2022-06-26T09:10:30.657832","exception":false,"start_time":"2022-06-26T08:50:21.192788","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:35:06.925468Z","iopub.execute_input":"2022-06-29T17:35:06.926149Z","iopub.status.idle":"2022-06-29T17:57:04.579772Z","shell.execute_reply.started":"2022-06-29T17:35:06.925925Z","shell.execute_reply":"2022-06-29T17:57:04.578710Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluate : Evaluate the model on Test Data and visualize results ","metadata":{"id":"uC2oS6V3c8tD","papermill":{"duration":0.293933,"end_time":"2022-06-26T09:10:31.333269","exception":false,"start_time":"2022-06-26T09:10:31.039336","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.load_state_dict(torch.load(f'{output_path}/{MODEL_NAME}.pt', map_location=device))\n_, test_metric = evaluate_model(model, test_dataloader, criterion, meanIoU, device)\nprint(f\"\\nModel has {test_metric} mean IoU in test set\")","metadata":{"id":"E3Jq4ac8c8tD","papermill":{"duration":10.553859,"end_time":"2022-06-26T09:10:42.184389","exception":false,"start_time":"2022-06-26T09:10:31.630530","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:57:04.581566Z","iopub.execute_input":"2022-06-29T17:57:04.582570Z","iopub.status.idle":"2022-06-29T17:57:14.997309Z","shell.execute_reply.started":"2022-06-29T17:57:04.582529Z","shell.execute_reply":"2022-06-29T17:57:14.996240Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def visualizePredictions(model : torch.nn.Module, dataSet : Dataset,  \n                         device :torch.device, numTestSamples : int):\n    \"\"\"Function visualizes predictions of input model on samples from\n    cityscapes dataset provided\n\n    Args:\n        model (torch.nn.Module): model whose output we're to visualize\n        dataSet (Dataset): dataset to take samples from\n        device (torch.device): compute device as in GPU, CPU etc\n        numTestSamples (int): number of samples to plot\n    \"\"\"\n    model.to(device=device)\n    model.eval()\n\n    # predictions on random samples\n    testSamples = np.random.choice(len(dataSet), numTestSamples).tolist()\n    _, axes = plt.subplots(numTestSamples, 3, figsize=(3*6, numTestSamples * 4))\n    \n    for i, sampleID in enumerate(testSamples):\n        inputImage, gt = dataSet[sampleID]\n\n        # input rgb image   \n        inputImage = inputImage.to(device)\n        landscape = inverse_transform(inputImage).permute(1, 2, 0).cpu().detach().numpy()\n        axes[i, 0].imshow(landscape)\n        axes[i, 0].set_title(\"Landscape\")\n\n        # groundtruth label image\n        label_class = gt.cpu().detach().numpy()\n        axes[i, 1].imshow(train_id_to_color[label_class])\n        axes[i, 1].set_title(\"Groudtruth Label\")\n\n        # predicted label image\n        y_pred = torch.argmax(model(inputImage.unsqueeze(0)), dim=1).squeeze(0)\n        label_class_predicted = y_pred.cpu().detach().numpy()    \n        axes[i, 2].imshow(train_id_to_color[label_class_predicted])\n        axes[i, 2].set_title(\"Predicted Label\")\n\n    plt.show()","metadata":{"id":"IJtD39Sxc8tE","papermill":{"duration":0.315446,"end_time":"2022-06-26T09:10:42.799701","exception":false,"start_time":"2022-06-26T09:10:42.484255","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:57:14.998950Z","iopub.execute_input":"2022-06-29T17:57:15.000943Z","iopub.status.idle":"2022-06-29T17:57:15.012594Z","shell.execute_reply.started":"2022-06-29T17:57:15.000896Z","shell.execute_reply":"2022-06-29T17:57:15.011552Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"visualizePredictions(model, test_set, device, numTestSamples=2)","metadata":{"id":"7-qBVW0Ec8tF","papermill":{"duration":1.570233,"end_time":"2022-06-26T09:10:44.663769","exception":false,"start_time":"2022-06-26T09:10:43.093536","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:57:15.013867Z","iopub.execute_input":"2022-06-29T17:57:15.014680Z","iopub.status.idle":"2022-06-29T17:57:15.990913Z","shell.execute_reply.started":"2022-06-29T17:57:15.014620Z","shell.execute_reply":"2022-06-29T17:57:15.989898Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Test on sample video","metadata":{"id":"jVs_u2PaoJa0","papermill":{"duration":0.298323,"end_time":"2022-06-26T09:10:45.293839","exception":false,"start_time":"2022-06-26T09:10:44.995516","status":"completed"},"tags":[]}},{"cell_type":"code","source":"input_video_path = f'{dataset_path}/bdd100k_test_{targetWidth}_{targetHeight}.avi'\noutput_video_path = f'{output_path}/{MODEL_NAME}_output_{targetWidth}_{targetHeight}.avi'\n\n# handles for input output videos\ninput_handle = cv2.VideoCapture(input_video_path)\noutput_handle = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), 5, (targetWidth, targetHeight))\n\n# create progress bar\nnum_frames = int(input_handle.get(cv2.CAP_PROP_FRAME_COUNT))\npbar = tqdm(total = num_frames, position=0, leave=True)\n\nwhile(input_handle.isOpened()):\n    ret, frame = input_handle.read()\n    if ret == True:\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # create torch tensor to give as input to model\n        pt_image = preprocess(frame)\n        pt_image = pt_image.to(device)\n\n        # get model prediction and convert to corresponding color\n        y_pred = torch.argmax(model(pt_image.unsqueeze(0)), dim=1).squeeze(0)\n        predicted_labels = y_pred.cpu().detach().numpy()\n        cm_labels = (train_id_to_color[predicted_labels]).astype(np.uint8)\n\n        # overlay prediction over input frame\n        overlay_image = cv2.addWeighted(frame, 1, cm_labels, 0.25, 0)\n        overlay_image = cv2.cvtColor(overlay_image, cv2.COLOR_RGB2BGR)\n\n        # write output result and update progress\n        output_handle.write(overlay_image)\n        pbar.update(1)\n\n    else:\n        break\n\noutput_handle.release()\ninput_handle.release()","metadata":{"id":"dEnKl73NoGvp","papermill":{"duration":5.021704,"end_time":"2022-06-26T09:10:50.616161","exception":false,"start_time":"2022-06-26T09:10:45.594457","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-29T17:57:15.992255Z","iopub.execute_input":"2022-06-29T17:57:15.993929Z","iopub.status.idle":"2022-06-29T17:57:20.310698Z","shell.execute_reply.started":"2022-06-29T17:57:15.993886Z","shell.execute_reply":"2022-06-29T17:57:20.309796Z"},"trusted":true},"execution_count":21,"outputs":[]}]}