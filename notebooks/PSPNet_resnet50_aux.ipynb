{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Welcome to the `PSPNet` Workshop!\nIn this workshop, we'll learn the concept of how to use PSPNet (Pyramid Scene Parsing Network) for Semantic Segmentation using Pytorch. We'll do the following tasks:\n\n- Dataset : Download and use BDD100k dataset\n- Network : Define PSPNet model using resnet50 Pyramid Pooling Module architecture and Auxilary loss\n- Training : Train and validate model on the custom dataset\n- Evaluate : Evaluate the model on Test Data and visualize results","metadata":{"id":"LkfoFeGlYxHY"}},{"cell_type":"code","source":"try:\n    import segmentation_models_pytorch as smp\nexcept:\n    !pip install segmentation-models-pytorch\n    import segmentation_models_pytorch as smp","metadata":{"id":"-njz3v-jGxsi","outputId":"77838d12-01f5-4ab1-d1d8-8bbe08bf2cc1","execution":{"iopub.status.busy":"2022-06-26T09:17:43.029186Z","iopub.execute_input":"2022-06-26T09:17:43.029996Z","iopub.status.idle":"2022-06-26T09:17:47.009185Z","shell.execute_reply.started":"2022-06-26T09:17:43.029892Z","shell.execute_reply":"2022-06-26T09:17:47.007785Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Basic Imports","metadata":{"id":"rKDStTzOQ_3R"}},{"cell_type":"code","source":"# basic imports\nimport random\nimport numpy as np\n\n# DL library imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n# libraries for loading image, plotting \nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"id":"-73t7omkQ-y7","execution":{"iopub.status.busy":"2022-06-26T09:17:47.011377Z","iopub.execute_input":"2022-06-26T09:17:47.012097Z","iopub.status.idle":"2022-06-26T09:17:47.042399Z","shell.execute_reply.started":"2022-06-26T09:17:47.012036Z","shell.execute_reply":"2022-06-26T09:17:47.041415Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 1. Dataset : Download and use BDD100k dataset","metadata":{"id":"1G6V9TIjsZB7"}},{"cell_type":"code","source":"ENVIRONMENT = 'kaggle'\n\nif ENVIRONMENT == 'kaggle':\n    dataset_path = '../input/image-segmentation'\n    output_path = '.'\n    \nelif ENVIRONMENT == 'colab':\n    import os\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=False)\n    os.chdir(\"/content/drive/My Drive/thinkAutonomous/image_segmentation\")\n    dataset_path = 'dataset'\n    output_path = 'dataset'\n    \nelse:\n    raise NotImplementedError(\"Env can be kaggle or colab\")","metadata":{"execution":{"iopub.status.busy":"2022-06-26T09:17:47.043691Z","iopub.execute_input":"2022-06-26T09:17:47.044226Z","iopub.status.idle":"2022-06-26T09:17:47.050819Z","shell.execute_reply.started":"2022-06-26T09:17:47.044186Z","shell.execute_reply":"2022-06-26T09:17:47.049543Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"targetWidth = 320\ntargetHeight = 180\n\n# batch size for data loaders\nTRAIN_BATCH_SIZE = 8\nTEST_BATCH_SIZE  = 8\n\n# Hyperparameters\nN_EPOCHS = 10\nNUM_CLASSES = 3\nMAX_LR = 3e-4\nMODEL_NAME = 'PSPNet_resnet50_aux'","metadata":{"id":"adLoblQrpzzy","outputId":"695af3d8-f020-4f37-9734-dcf9f0163229","execution":{"iopub.status.busy":"2022-06-26T09:17:47.053966Z","iopub.execute_input":"2022-06-26T09:17:47.054386Z","iopub.status.idle":"2022-06-26T09:17:47.061852Z","shell.execute_reply.started":"2022-06-26T09:17:47.054350Z","shell.execute_reply":"2022-06-26T09:17:47.060970Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"images = np.load(f'{dataset_path}/image_{targetHeight}_{targetWidth}.npy')\nlabels = np.load(f'{dataset_path}/label_{targetHeight}_{targetWidth}.npy')\nprint(f\"RGB images shape = {images.shape}, Label images shape = {labels.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-26T09:17:47.063576Z","iopub.execute_input":"2022-06-26T09:17:47.063968Z","iopub.status.idle":"2022-06-26T09:17:47.337272Z","shell.execute_reply.started":"2022-06-26T09:17:47.063933Z","shell.execute_reply":"2022-06-26T09:17:47.336064Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Torch Dataset definition","metadata":{"id":"qmFiD3i-0Crd"}},{"cell_type":"code","source":"class BDD100k_dataset(Dataset):\n    def __init__(self, images, labels, tf=None):\n        \"\"\"Dataset class for BDD100k_dataset drivable / segmentation data \"\"\"\n        self.images = images\n        self.labels = labels\n        self.tf = tf\n    \n    def __len__(self):\n        return self.images.shape[0]\n  \n    def __getitem__(self, index):\n        # read source image and convert to RGB, apply transform\n        rgb_image = self.images[index]\n        if self.tf is not None:\n            rgb_image = self.tf(rgb_image)\n\n        # read label image and convert to torch tensor\n        label_image  = torch.from_numpy(self.labels[index]).long()\n        return rgb_image, label_image  ","metadata":{"id":"ySpXTOvpmPk8","execution":{"iopub.status.busy":"2022-06-26T09:17:47.338970Z","iopub.execute_input":"2022-06-26T09:17:47.339395Z","iopub.status.idle":"2022-06-26T09:17:47.347776Z","shell.execute_reply.started":"2022-06-26T09:17:47.339353Z","shell.execute_reply":"2022-06-26T09:17:47.346181Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Convert to torch tensor and normalize images using Imagenet values\npreprocess = transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=(0.485, 0.56, 0.406), std=(0.229, 0.224, 0.225))\n                ])\n\ndata = BDD100k_dataset(images, labels, tf=preprocess)","metadata":{"id":"r6XW6HajnzPq","execution":{"iopub.status.busy":"2022-06-26T09:17:47.350047Z","iopub.execute_input":"2022-06-26T09:17:47.350493Z","iopub.status.idle":"2022-06-26T09:17:47.359499Z","shell.execute_reply.started":"2022-06-26T09:17:47.350455Z","shell.execute_reply":"2022-06-26T09:17:47.358584Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Splitting Training data into train and validation sets, creating Dataloaders","metadata":{"id":"rB9JLxjfoENJ"}},{"cell_type":"code","source":"# split train data into train, validation and test sets\ntotal_count = len(data)\ntrain_count = int(0.7 * total_count) \nvalid_count = int(0.2 * total_count)\ntest_count = total_count - train_count - valid_count\ntrain_set, val_set, test_set = torch.utils.data.random_split(data, \n            (train_count, valid_count, test_count), generator=torch.Generator().manual_seed(1))\n\ntrain_dataloader = DataLoader(train_set, batch_size=TRAIN_BATCH_SIZE,drop_last=True)\nval_dataloader   = DataLoader(val_set, batch_size=TEST_BATCH_SIZE)\ntest_dataloader  = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)","metadata":{"id":"KKXj9c2qoC40","execution":{"iopub.status.busy":"2022-06-26T09:17:47.362029Z","iopub.execute_input":"2022-06-26T09:17:47.362845Z","iopub.status.idle":"2022-06-26T09:17:47.370445Z","shell.execute_reply.started":"2022-06-26T09:17:47.362807Z","shell.execute_reply":"2022-06-26T09:17:47.369506Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Let's verify size of images from the dataset","metadata":{"id":"ku1BYOyXosMD"}},{"cell_type":"code","source":"sample_image, sample_label = train_set[0]\nprint(f\"There are {len(train_set)} train images, {len(val_set)} validation images, {len(test_set)} test Images\")\nprint(f\"Input shape = {sample_image.shape}, output label shape = {sample_label.shape}\")","metadata":{"id":"yWrMEcQQota2","outputId":"088865c6-546f-4b68-a0b0-4a35a99d03f9","execution":{"iopub.status.busy":"2022-06-26T09:17:47.372198Z","iopub.execute_input":"2022-06-26T09:17:47.372635Z","iopub.status.idle":"2022-06-26T09:17:47.384580Z","shell.execute_reply.started":"2022-06-26T09:17:47.372601Z","shell.execute_reply":"2022-06-26T09:17:47.383235Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Show Sample images from dataset","metadata":{"id":"MhMFn7YWprGr"}},{"cell_type":"code","source":"# reference : https://github.com/bdd100k/bdd100k/blob/master/bdd100k/label/label.py\nfrom collections import namedtuple\nLabel = namedtuple( \"Label\", [ \"name\", \"train_id\", \"color\"])\ndrivables = [ \n             Label(\"direct\", 0, (219, 94, 86)),        # red\n             Label(\"alternative\", 1, (86, 211, 219)),  # cyan\n             Label(\"background\", 2, (0, 0, 0)),        # black          \n            ]\ntrain_id_to_color = [c.color for c in drivables if (c.train_id != -1 and c.train_id != 255)]\ntrain_id_to_color = np.array(train_id_to_color)","metadata":{"id":"jOQpj7kfq_tM","execution":{"iopub.status.busy":"2022-06-26T09:17:47.388730Z","iopub.execute_input":"2022-06-26T09:17:47.388979Z","iopub.status.idle":"2022-06-26T09:17:47.395840Z","shell.execute_reply.started":"2022-06-26T09:17:47.388956Z","shell.execute_reply":"2022-06-26T09:17:47.394702Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# when using torch datasets we defined earlier, the output image\n# is normalized. So we're defining an inverse transformation to \n# transform to normal RGB format\ninverse_transform = transforms.Compose([\n        transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n    ])\n\nrgb_image, label = train_set[random.randint(0, len(train_set))]\nrgb_image = inverse_transform(rgb_image).permute(1, 2, 0).cpu().detach().numpy()\nlabel = label.cpu().detach().numpy()\n\n# plot sample image\nfig, axes = plt.subplots(1,2, figsize=(20,10))\naxes[0].imshow(rgb_image);\naxes[0].set_title(\"Image\");\naxes[0].axis('off');\naxes[1].imshow(train_id_to_color[label]);\naxes[1].set_title(\"Label\");\naxes[1].axis('off');","metadata":{"id":"UneBdP1LvZQr","outputId":"cf00c661-2725-4604-bc6c-ce4a72f11139","execution":{"iopub.status.busy":"2022-06-26T09:17:47.398074Z","iopub.execute_input":"2022-06-26T09:17:47.398955Z","iopub.status.idle":"2022-06-26T09:17:47.714533Z","shell.execute_reply.started":"2022-06-26T09:17:47.398913Z","shell.execute_reply":"2022-06-26T09:17:47.712946Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Network : Define PSPNet model using resnet50 Pyramid Pooling Module architecture","metadata":{"id":"zbOYP8UHc8s3"}},{"cell_type":"code","source":"class pyramid_pooling_module(nn.Module):\n    def __init__(self, in_channels, out_channels, bin_sizes):\n        super(pyramid_pooling_module, self).__init__()\n        \n        # create pyramid pooling layers for each level\n        self.pyramid_pool_layers = []\n        for bin_sz in bin_sizes:\n            self.pyramid_pool_layers.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(bin_sz),\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            ))\n        self.pyramid_pool_layers = nn.ModuleList(self.pyramid_pool_layers)\n\n    def forward(self, x):\n        x_size = x.size()\n        out = [x]\n        for layer in self.pyramid_pool_layers:\n            out.append(F.interpolate(layer(x), x_size[2:], mode='bilinear', align_corners=True))\n        return torch.cat(out, 1)\n    \n\nfrom torchvision.models import resnet50\n\nclass PSPNet(nn.Module):\n    def __init__(self, in_channels, num_classes, use_aux=False):\n        super(PSPNet, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n                \n        # backbone layers\n        backbone = resnet50(pretrained=True, replace_stride_with_dilation=[False, True, True])        \n        self.initial = nn.Sequential(*list(backbone.children())[:4])\n        self.layer1 = backbone.layer1\n        self.layer2 = backbone.layer2\n        self.layer3 = backbone.layer3\n        self.layer4 = backbone.layer4\n        \n        # Pyramid pooling module components\n        ppm_in_channels = int(backbone.fc.in_features)\n        self.ppm = pyramid_pooling_module(in_channels=ppm_in_channels, \n                                     out_channels=512, bin_sizes=[1,2,3,6])\n        \n        # classifier head\n        self.cls = nn.Sequential(\n            nn.Conv2d(ppm_in_channels * 2, 512, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(p=0.1),\n            nn.Conv2d(512,  self.num_classes, kernel_size=1)            \n        )\n        \n        # main branch is composed of PPM + Classifier\n        self.main_branch = nn.Sequential(self.ppm, self.cls)\n        \n        # Define Auxilary branch if specified\n        self.use_aux = False\n        if(self.training and use_aux):\n            self.use_aux = True\n            self.aux_branch = nn.Sequential(\n                nn.Conv2d( int(ppm_in_channels / 2) , 256, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(256),\n                nn.ReLU(inplace=True),\n                nn.Dropout2d(p=0.1),\n                nn.Conv2d(256, self.num_classes, kernel_size=1)\n            )\n        \n        \n    def forward(self, x):\n        input_size = x.shape[-2:]\n        \n        # Pass input through Backbone layers\n        x = self.initial(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x_aux = self.layer3(x)\n        x = self.layer4(x_aux)\n        \n        # Get Main branch output\n        main_output = self.main_branch(x)\n        main_output = F.interpolate(main_output, size=input_size, mode='bilinear')\n        \n        # If needed, get auxiliary branch output\n        if(self.training and self.use_aux):\n            aux_output = F.interpolate(self.aux_branch(x_aux), size=input_size, mode='bilinear')\n            output = {}\n            output['aux'] = aux_output\n            output['main'] = main_output\n            return output\n        \n        return main_output","metadata":{"execution":{"iopub.status.busy":"2022-06-26T09:17:47.715595Z","iopub.execute_input":"2022-06-26T09:17:47.715906Z","iopub.status.idle":"2022-06-26T09:17:47.738199Z","shell.execute_reply.started":"2022-06-26T09:17:47.715876Z","shell.execute_reply":"2022-06-26T09:17:47.737119Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 3. Training : Train and validate model on the custom dataset","metadata":{"id":"HeI63CaFc8s6"}},{"cell_type":"markdown","source":"\nBefore we train our model, we'll define some helper functions to calculate metric, plot training results etc","metadata":{"id":"4hBQ5Xmuc8s7"}},{"cell_type":"markdown","source":"## Metric - meanIoU","metadata":{"id":"p2saoCUQc8s8"}},{"cell_type":"code","source":"class meanIoU:\n    \"\"\" Class to find the mean IoU using confusion matrix approach \"\"\"    \n    def __init__(self, num_classes):\n        self.iou_metric = 0.0\n        self.num_classes = num_classes\n        # placeholder for confusion matrix on entire dataset\n        self.confusion_matrix = np.zeros((self.num_classes, self.num_classes))\n\n    def update(self, y_preds, labels):\n        \"\"\" Function finds the IoU for the input batch\n        and add batch metrics to overall metrics \"\"\"\n        predicted_labels = torch.argmax(y_preds, dim=1)\n        batch_confusion_matrix = self._fast_hist(labels.numpy().flatten(), predicted_labels.numpy().flatten())\n        self.confusion_matrix += batch_confusion_matrix\n    \n    def _fast_hist(self, label_true, label_pred):\n        \"\"\" Function to calculate confusion matrix on single batch \"\"\"\n        mask = (label_true >= 0) & (label_true < self.num_classes)\n        hist = np.bincount(\n            self.num_classes * label_true[mask].astype(int) + label_pred[mask],\n            minlength=self.num_classes ** 2,\n        ).reshape(self.num_classes, self.num_classes)\n        return hist\n\n    def compute(self):\n        \"\"\" Computes overall meanIoU metric from confusion matrix data \"\"\" \n        hist = self.confusion_matrix\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu)\n        return mean_iu\n\n    def reset(self):\n        self.iou_metric = 0.0\n        self.confusion_matrix = np.zeros((self.num_classes, self.num_classes))","metadata":{"id":"MRyngm4nc8s8","execution":{"iopub.status.busy":"2022-06-26T09:17:47.739818Z","iopub.execute_input":"2022-06-26T09:17:47.740198Z","iopub.status.idle":"2022-06-26T09:17:47.753001Z","shell.execute_reply.started":"2022-06-26T09:17:47.740160Z","shell.execute_reply":"2022-06-26T09:17:47.751927Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Function to plot training curves","metadata":{"id":"tZBAn9xXc8s9"}},{"cell_type":"code","source":"def plot_training_results(df, model_name):\n    fig, ax1 = plt.subplots(figsize=(10,4))\n    ax1.set_ylabel('trainLoss', color='tab:red')\n    ax1.plot(df['epoch'].values, df['trainLoss'].values, color='tab:red')\n    ax1.tick_params(axis='y', labelcolor='tab:red')\n\n    ax2 = ax1.twinx()  \n    ax2.set_ylabel('validationLoss', color='tab:blue')\n    ax2.plot(df['epoch'].values, df['validationLoss'].values, color='tab:blue')\n    ax2.tick_params(axis='y', labelcolor='tab:blue')\n\n    plt.suptitle(f'{model_name} Training, Validation Curves')\n    plt.show()","metadata":{"id":"2Ft6-lRuc8s-","execution":{"iopub.status.busy":"2022-06-26T09:17:47.755019Z","iopub.execute_input":"2022-06-26T09:17:47.756350Z","iopub.status.idle":"2022-06-26T09:17:47.765968Z","shell.execute_reply.started":"2022-06-26T09:17:47.756314Z","shell.execute_reply":"2022-06-26T09:17:47.765058Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Train validate function","metadata":{"id":"Ye0G3dMEc8s-"}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\ndef evaluate_model(model, dataloader, criterion, metric_class, device):\n    model.eval()\n    total_loss = 0.0\n    metric_object = metric_class(NUM_CLASSES)\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, total=len(dataloader), position=0, leave=True):\n            inputs = inputs.to(device)\n            labels = labels.to(device)                \n            y_preds = model(inputs)\n\n            # calculate loss\n            loss = criterion(y_preds, labels)\n            total_loss += loss.item()\n\n            # update batch metric information            \n            metric_object.update(y_preds.cpu().detach(), labels.cpu().detach())\n\n    evaluation_loss = total_loss / len(dataloader)\n    evaluation_metric = metric_object.compute()\n    return evaluation_loss, evaluation_metric","metadata":{"id":"rq5tJoYXc8s_","execution":{"iopub.status.busy":"2022-06-26T09:17:47.768217Z","iopub.execute_input":"2022-06-26T09:17:47.770078Z","iopub.status.idle":"2022-06-26T09:17:47.780500Z","shell.execute_reply.started":"2022-06-26T09:17:47.770037Z","shell.execute_reply":"2022-06-26T09:17:47.779529Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train_validate_model(model, num_epochs, model_name, criterion, optimizer, \n                         device, dataloader_train, dataloader_valid, \n                         metric_class, metric_name, lr_scheduler = None):\n    # initialize placeholders for running values\n    results = []    \n    min_val_loss = np.Inf\n    len_train_loader = len(dataloader_train)\n\n    # move model to device\n    model.to(device)\n    \n    for epoch in range(num_epochs):\n        print(f\"Starting {epoch + 1} epoch ...\")\n        \n        # Training\n        model.train()\n        train_loss = 0.0\n        for inputs, labels in tqdm(dataloader_train, total=len_train_loader,position=0, leave=True):\n            inputs = inputs.to(device)\n            labels = labels.to(device) \n\n            # Forward pass\n            y_preds = model(inputs)\n            loss = criterion(y_preds, labels)\n            train_loss += loss.item()\n              \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # adjust learning rate\n            if lr_scheduler is not None:\n                lr_scheduler.step()\n            \n        # compute per batch losses, metric value\n        train_loss = train_loss / len(dataloader_train)\n        validation_loss, validation_metric = evaluate_model(\n                        model, dataloader_valid, criterion, metric_class, device)\n\n        print(f'Epoch: {epoch+1}, trainLoss:{train_loss:6.5f}, validationLoss:{validation_loss:6.5f}, {metric_name}:{validation_metric: 4.2f}')\n        \n        # store results\n        results.append({'epoch': epoch, \n                        'trainLoss': train_loss, \n                        'validationLoss': validation_loss, \n                        f'{metric_name}': validation_metric})\n        \n        # if validation loss has decreased, save model and reset variable\n        if validation_loss <= min_val_loss:\n            min_val_loss = validation_loss\n            torch.save(model.state_dict(), f\"{output_path}/{model_name}.pt\")\n\n    # plot results\n    results = pd.DataFrame(results)\n    plot_training_results(results, model_name)\n    return results","metadata":{"id":"oSX6yKZQc8tA","execution":{"iopub.status.busy":"2022-06-26T09:17:47.783159Z","iopub.execute_input":"2022-06-26T09:17:47.783970Z","iopub.status.idle":"2022-06-26T09:17:47.795966Z","shell.execute_reply.started":"2022-06-26T09:17:47.783932Z","shell.execute_reply":"2022-06-26T09:17:47.795019Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{"id":"cCnjwooT0Rde"}},{"cell_type":"code","source":"class pspnet_loss(nn.Module):\n    def __init__(self, aux_weight):\n        super(pspnet_loss, self).__init__()\n        self.loss_fn = smp.losses.DiceLoss('multiclass', classes=[0,1,2], log_loss = True, smooth=1.0)\n        self.aux_weight = aux_weight\n    \n    def forward(self, preds, labels):\n        if(isinstance(preds, dict) == True):\n            main_loss = self.loss_fn(preds['main'], labels)\n            aux_loss = self.loss_fn(preds['aux'], labels)\n            loss = (1 - self.aux_weight) * main_loss + self.aux_weight * aux_loss\n        else:\n            loss = self.loss_fn(preds, labels)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-06-26T09:17:47.797764Z","iopub.execute_input":"2022-06-26T09:17:47.798550Z","iopub.status.idle":"2022-06-26T09:17:47.807844Z","shell.execute_reply.started":"2022-06-26T09:17:47.798513Z","shell.execute_reply":"2022-06-26T09:17:47.806865Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\n\ncriterion = pspnet_loss(aux_weight=0.4)\n\n# create model, optimizer, lr_scheduler and pass to training function\nmodel = PSPNet(in_channels=3, num_classes=NUM_CLASSES, use_aux=True).to(device)\noptimizer = optim.Adam(model.parameters(), lr=MAX_LR)\nscheduler = OneCycleLR(optimizer, max_lr= MAX_LR, epochs = N_EPOCHS,steps_per_epoch = len(train_dataloader), \n                       pct_start=0.3, div_factor=10, anneal_strategy='cos')\n\n_ = train_validate_model(model, N_EPOCHS, MODEL_NAME, criterion, optimizer, \n                         device, train_dataloader, val_dataloader, meanIoU, 'meanIoU',\n                         lr_scheduler = scheduler)","metadata":{"outputId":"1c9d2251-1818-4467-9f74-ecd3731f8144","id":"X2ubU3A2c8tB","execution":{"iopub.status.busy":"2022-06-26T09:17:47.809325Z","iopub.execute_input":"2022-06-26T09:17:47.810082Z","iopub.status.idle":"2022-06-26T09:23:50.476409Z","shell.execute_reply.started":"2022-06-26T09:17:47.810044Z","shell.execute_reply":"2022-06-26T09:23:50.475234Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluate : Evaluate the model on Test Data and visualize results ","metadata":{"id":"uC2oS6V3c8tD"}},{"cell_type":"code","source":"model.load_state_dict(torch.load(f'{output_path}/{MODEL_NAME}.pt', map_location=device))\n_, test_metric = evaluate_model(model, test_dataloader, criterion, meanIoU, device)\nprint(f\"\\nModel has {test_metric} mean IoU in test set\")","metadata":{"id":"E3Jq4ac8c8tD","execution":{"iopub.status.busy":"2022-06-26T09:23:50.477984Z","iopub.execute_input":"2022-06-26T09:23:50.478312Z","iopub.status.idle":"2022-06-26T09:24:00.007559Z","shell.execute_reply.started":"2022-06-26T09:23:50.478278Z","shell.execute_reply":"2022-06-26T09:24:00.006524Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def visualizePredictions(model : torch.nn.Module, dataSet : Dataset,  \n                         device :torch.device, numTestSamples : int):\n    \"\"\"Function visualizes predictions of input model on samples from\n    cityscapes dataset provided\n\n    Args:\n        model (torch.nn.Module): model whose output we're to visualize\n        dataSet (Dataset): dataset to take samples from\n        device (torch.device): compute device as in GPU, CPU etc\n        numTestSamples (int): number of samples to plot\n    \"\"\"\n    model.to(device=device)\n    model.eval()\n\n    # predictions on random samples\n    testSamples = np.random.choice(len(dataSet), numTestSamples).tolist()\n    _, axes = plt.subplots(numTestSamples, 3, figsize=(3*6, numTestSamples * 4))\n    \n    for i, sampleID in enumerate(testSamples):\n        inputImage, gt = dataSet[sampleID]\n\n        # input rgb image   \n        inputImage = inputImage.to(device)\n        landscape = inverse_transform(inputImage).permute(1, 2, 0).cpu().detach().numpy()\n        axes[i, 0].imshow(landscape)\n        axes[i, 0].set_title(\"Landscape\")\n\n        # groundtruth label image\n        label_class = gt.cpu().detach().numpy()\n        axes[i, 1].imshow(train_id_to_color[label_class])\n        axes[i, 1].set_title(\"Groudtruth Label\")\n\n        # predicted label image\n        y_pred = torch.argmax(model(inputImage.unsqueeze(0)), dim=1).squeeze(0)\n        label_class_predicted = y_pred.cpu().detach().numpy()    \n        axes[i, 2].imshow(train_id_to_color[label_class_predicted])\n        axes[i, 2].set_title(\"Predicted Label\")\n\n    plt.show()","metadata":{"id":"IJtD39Sxc8tE","execution":{"iopub.status.busy":"2022-06-26T09:24:00.009350Z","iopub.execute_input":"2022-06-26T09:24:00.010060Z","iopub.status.idle":"2022-06-26T09:24:00.023044Z","shell.execute_reply.started":"2022-06-26T09:24:00.010018Z","shell.execute_reply":"2022-06-26T09:24:00.022050Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"visualizePredictions(model, test_set, device, numTestSamples=2)","metadata":{"id":"7-qBVW0Ec8tF","execution":{"iopub.status.busy":"2022-06-26T09:24:00.024540Z","iopub.execute_input":"2022-06-26T09:24:00.024909Z","iopub.status.idle":"2022-06-26T09:24:01.239096Z","shell.execute_reply.started":"2022-06-26T09:24:00.024872Z","shell.execute_reply":"2022-06-26T09:24:01.238155Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Test on sample video","metadata":{"id":"jVs_u2PaoJa0"}},{"cell_type":"code","source":"input_video_path = f'{dataset_path}/bdd100k_test_{targetWidth}_{targetHeight}.avi'\noutput_video_path = f'{output_path}/{MODEL_NAME}_output_{targetWidth}_{targetHeight}.avi'\n\n# handles for input output videos\ninput_handle = cv2.VideoCapture(input_video_path)\noutput_handle = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), 5, (targetWidth, targetHeight))\n\n# create progress bar\nnum_frames = int(input_handle.get(cv2.CAP_PROP_FRAME_COUNT))\npbar = tqdm(total = num_frames, position=0, leave=True)\n\nwhile(input_handle.isOpened()):\n    ret, frame = input_handle.read()\n    if ret == True:\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # create torch tensor to give as input to model\n        pt_image = preprocess(frame)\n        pt_image = pt_image.to(device)\n\n        # get model prediction and convert to corresponding color\n        y_pred = torch.argmax(model(pt_image.unsqueeze(0)), dim=1).squeeze(0)\n        predicted_labels = y_pred.cpu().detach().numpy()\n        cm_labels = (train_id_to_color[predicted_labels]).astype(np.uint8)\n\n        # overlay prediction over input frame\n        overlay_image = cv2.addWeighted(frame, 1, cm_labels, 0.25, 0)\n        overlay_image = cv2.cvtColor(overlay_image, cv2.COLOR_RGB2BGR)\n\n        # write output result and update progress\n        output_handle.write(overlay_image)\n        pbar.update(1)\n\n    else:\n        break\n\noutput_handle.release()\ninput_handle.release()","metadata":{"id":"dEnKl73NoGvp","execution":{"iopub.status.busy":"2022-06-26T09:24:01.240697Z","iopub.execute_input":"2022-06-26T09:24:01.241454Z","iopub.status.idle":"2022-06-26T09:24:05.143641Z","shell.execute_reply.started":"2022-06-26T09:24:01.241413Z","shell.execute_reply":"2022-06-26T09:24:05.142562Z"},"trusted":true},"execution_count":22,"outputs":[]}]}