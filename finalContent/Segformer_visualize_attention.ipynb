{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13f01f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:53:38.392561Z",
     "iopub.status.busy": "2022-10-03T03:53:38.392015Z",
     "iopub.status.idle": "2022-10-03T03:54:19.711960Z",
     "shell.execute_reply": "2022-10-03T03:54:19.710886Z"
    },
    "papermill": {
     "duration": 41.329968,
     "end_time": "2022-10-03T03:54:19.714976",
     "exception": false,
     "start_time": "2022-10-03T03:53:38.385008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\r\n",
      "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)\r\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (4.3.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.1.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (2.28.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.6)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (1.26.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (2022.6.15.2)\r\n",
      "Installing collected packages: timm\r\n",
      "Successfully installed timm-0.6.7\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting einops\r\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\r\n",
      "Installing collected packages: einops\r\n",
      "Successfully installed einops-0.4.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting segmentation-models-pytorch\r\n",
      "  Downloading segmentation_models_pytorch-0.3.0-py3-none-any.whl (97 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m700.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting efficientnet-pytorch==0.7.1\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting timm==0.4.12\r\n",
      "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (4.64.0)\r\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (0.12.0)\r\n",
      "Collecting pretrainedmodels==0.7.4\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch) (9.1.1)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.11.0)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (4.3.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.28.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.21.6)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.6.15.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.12)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=2b59506083c9e0d0cd937d6d468a87411d05ecc7194dbe3b08638e435262af0b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=678a5f919876b35bc955af160da23683797efead5f865daeae6057d9519d1100\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 0.6.7\r\n",
      "    Uninstalling timm-0.6.7:\r\n",
      "      Successfully uninstalled timm-0.6.7\r\n",
      "Successfully installed efficientnet-pytorch-0.7.1 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.0 timm-0.4.12\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "\n",
    "try:\n",
    "    from einops import rearrange\n",
    "    import segmentation_models_pytorch as smp\n",
    "    from timm.models.layers import drop_path, trunc_normal_\n",
    "    \n",
    "except:\n",
    "    !pip install timm\n",
    "    !pip install einops\n",
    "    !pip install segmentation-models-pytorch\n",
    "    \n",
    "    from einops import rearrange\n",
    "    import segmentation_models_pytorch as smp\n",
    "    from timm.models.layers import drop_path, trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd24122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:19.731683Z",
     "iopub.status.busy": "2022-10-03T03:54:19.731181Z",
     "iopub.status.idle": "2022-10-03T03:54:19.780963Z",
     "shell.execute_reply": "2022-10-03T03:54:19.780000Z"
    },
    "papermill": {
     "duration": 0.060733,
     "end_time": "2022-10-03T03:54:19.782958",
     "exception": false,
     "start_time": "2022-10-03T03:54:19.722225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class overlap_patch_embed(nn.Module):\n",
    "    def __init__(self, patch_size, stride, in_chans, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
    "                              padding=(patch_size // 2, patch_size // 2))\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        return x, h, w\n",
    "    \n",
    "    \n",
    "\n",
    "class mix_feedforward(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, dropout_p = 0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        \n",
    "        # Depth-wise separable convolution\n",
    "        self.conv = nn.Conv2d(hidden_features, hidden_features, (3, 3), padding=(1, 1),\n",
    "                              bias=True, groups=hidden_features)\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "    def forward(self, x, h, w):\n",
    "        x = self.fc1(x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        x = self.conv(x)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = F.gelu(x)\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class efficient_self_attention(nn.Module):\n",
    "    def __init__(self, attn_dim, num_heads, dropout_p, sr_ratio):\n",
    "        super().__init__()\n",
    "        assert attn_dim % num_heads == 0, f'expected attn_dim {attn_dim} to be a multiple of num_heads {num_heads}'\n",
    "        self.attn_dim = attn_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_p = dropout_p\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv2d(attn_dim, attn_dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(attn_dim)\n",
    "\n",
    "        # Multi-head Self-Attention using dot product\n",
    "        # Query - Key Dot product is scaled by root of head_dim\n",
    "        self.q = nn.Linear(attn_dim, attn_dim, bias=True)\n",
    "        self.kv = nn.Linear(attn_dim, attn_dim * 2, bias=True)\n",
    "        self.scale = (attn_dim // num_heads) ** -0.5\n",
    "\n",
    "        # Projecting concatenated outputs from \n",
    "        # multiple heads to single `attn_dim` size\n",
    "        self.proj = nn.Linear(attn_dim, attn_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        q = self.q(x)\n",
    "        q = rearrange(q, ('b hw (m c) -> b m hw c'), m=self.num_heads)\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "            x = self.sr(x)\n",
    "            x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "            x = self.norm(x)\n",
    "\n",
    "        x = self.kv(x)\n",
    "        x = rearrange(x, 'b d (a m c) -> a b m d c', a=2, m=self.num_heads)\n",
    "        k, v = x[0], x[1] # x.unbind(0)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = rearrange(x, 'b m hw c -> b hw (m c)')\n",
    "        x = self.proj(x)\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        \n",
    "        attn_output = {'key' : k, 'query' : q, 'value' : v, 'attn' : attn}\n",
    "        return x, attn_output\n",
    "    \n",
    "    \n",
    "\n",
    "class transformer_block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout_p, drop_path_p, sr_ratio):\n",
    "        super().__init__()\n",
    "        # One transformer block is defined as :\n",
    "        # Norm -> self-attention -> Norm -> FeedForward\n",
    "        # skip-connections are added after attention and FF layers\n",
    "        self.attn = efficient_self_attention(attn_dim=dim, num_heads=num_heads, \n",
    "                    dropout_p=dropout_p, sr_ratio=sr_ratio)\n",
    "        self.ffn = mix_feedforward( dim, dim, hidden_features=dim * 4, dropout_p=dropout_p)                    \n",
    "\n",
    "        self.drop_path_p = drop_path_p\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        \n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        # Norm -> self-attention\n",
    "        skip = x\n",
    "        x = self.norm1(x)\n",
    "        x, attn_output = self.attn(x, h, w)\n",
    "        x = drop_path(x, drop_prob=self.drop_path_p, training=self.training)\n",
    "        x = x + skip\n",
    "\n",
    "        # Norm -> FeedForward\n",
    "        skip = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x, h, w)\n",
    "        x = drop_path(x, drop_prob=self.drop_path_p, training=self.training)\n",
    "        x = x + skip\n",
    "        return x, attn_output\n",
    "    \n",
    "    \n",
    "    \n",
    "class mix_transformer_stage(nn.Module):\n",
    "    def __init__(self, patch_embed, blocks, norm):\n",
    "        super().__init__()\n",
    "        self.patch_embed = patch_embed\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        # patch embedding and store required data\n",
    "        stage_output  = {}\n",
    "        stage_output['patch_embed_input'] = x\n",
    "        x, h, w = self.patch_embed(x)\n",
    "        stage_output['patch_embed_h'] = h\n",
    "        stage_output['patch_embed_w'] = w\n",
    "        stage_output['patch_embed_output'] = x\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x, attn_output = block(x, h, w)\n",
    "                        \n",
    "        x = self.norm(x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        \n",
    "        # store last attention block data \n",
    "        # in stages' output data\n",
    "        for k,v in attn_output.items():\n",
    "            stage_output[k] = v\n",
    "        del attn_output\n",
    "        return x, stage_output\n",
    "    \n",
    "    \n",
    "    \n",
    "class mix_transformer(nn.Module):\n",
    "    def __init__(self, in_chans, embed_dims, num_heads, depths, \n",
    "                sr_ratios, dropout_p, drop_path_p):\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList()\n",
    "        for stage_i in range(len(depths)):\n",
    "            # Each Stage consists of following blocks :\n",
    "            # Overlap patch embedding -> mix_transformer_block -> norm\n",
    "            blocks = []\n",
    "            for i in range(depths[stage_i]):\n",
    "                blocks.append(transformer_block(dim = embed_dims[stage_i],\n",
    "                        num_heads= num_heads[stage_i], dropout_p=dropout_p,\n",
    "                        drop_path_p = drop_path_p * (sum(depths[:stage_i])+i) / (sum(depths)-1),\n",
    "                        sr_ratio = sr_ratios[stage_i] ))\n",
    "\n",
    "            if(stage_i == 0):\n",
    "                patch_size = 7\n",
    "                stride = 4\n",
    "                in_chans = in_chans\n",
    "            else:\n",
    "                patch_size = 3\n",
    "                stride = 2\n",
    "                in_chans = embed_dims[stage_i -1]\n",
    "            \n",
    "            patch_embed = overlap_patch_embed(patch_size, stride=stride, in_chans=in_chans, \n",
    "                            embed_dim= embed_dims[stage_i])\n",
    "            norm = nn.LayerNorm(embed_dims[stage_i], eps=1e-6)\n",
    "            self.stages.append(mix_transformer_stage(patch_embed, blocks, norm))\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i,stage in enumerate(self.stages):\n",
    "            x, _ = stage(x)\n",
    "            outputs.append(x)\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def get_attn_outputs(self, x):\n",
    "        stage_outputs = []\n",
    "        for i,stage in enumerate(self.stages):\n",
    "            x, stage_data = stage(x)\n",
    "            stage_outputs.append(stage_data)\n",
    "        return stage_outputs\n",
    "\n",
    "\n",
    "class segformer_head(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, embed_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # 1x1 conv to fuse multi-scale output from encoder\n",
    "        self.layers = nn.ModuleList([nn.Conv2d(chans, embed_dim, (1, 1))\n",
    "                                     for chans in reversed(in_channels)])\n",
    "        self.linear_fuse = nn.Conv2d(embed_dim * len(self.layers), embed_dim, (1, 1), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(embed_dim, eps=1e-5)\n",
    "\n",
    "        # 1x1 conv to get num_class channel predictions\n",
    "        self.linear_pred = nn.Conv2d(self.embed_dim, num_classes, kernel_size=(1, 1))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.kaiming_normal_(self.linear_fuse.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_size = x[0].shape[2:]\n",
    "        \n",
    "        # project each encoder stage output to H/4, W/4\n",
    "        x = [layer(xi) for layer, xi in zip(self.layers, reversed(x))]\n",
    "        x = [F.interpolate(xi, size=feature_size, mode='bilinear', align_corners=False)\n",
    "             for xi in x[:-1]] + [x[-1]]\n",
    "        \n",
    "        # concatenate project output and use 1x1\n",
    "        # convs to get num_class channel output\n",
    "        x = self.linear_fuse(torch.cat(x, dim=1))\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        x = self.linear_pred(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "class segformer_mit_b3(nn.Module):    \n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        # Encoder block    \n",
    "        self.backbone = mix_transformer(in_chans=in_channels, embed_dims=(64, 128, 320, 512), \n",
    "                                    num_heads=(1, 2, 5, 8), depths=(3, 4, 18, 3),\n",
    "                                    sr_ratios=(8, 4, 2, 1), dropout_p=0.0, drop_path_p=0.1)\n",
    "        # decoder block\n",
    "        self.decoder_head = segformer_head(in_channels=(64, 128, 320, 512), \n",
    "                                    num_classes=num_classes, embed_dim=256)\n",
    "        \n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            \n",
    "            \n",
    "    def forward(self, x):\n",
    "        image_hw = x.shape[2:]\n",
    "        x = self.backbone(x)\n",
    "        x = self.decoder_head(x)\n",
    "        x = F.interpolate(x, size=image_hw, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def get_attention_outputs(self, x):\n",
    "        return self.backbone.get_attn_outputs(x)\n",
    "    \n",
    "    def get_last_selfattention(self, x):\n",
    "        outputs = self.get_attention_outputs(x)\n",
    "        return outputs[-1].get('attn', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560601bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:19.797606Z",
     "iopub.status.busy": "2022-10-03T03:54:19.797033Z",
     "iopub.status.idle": "2022-10-03T03:54:19.804005Z",
     "shell.execute_reply": "2022-10-03T03:54:19.803146Z"
    },
    "papermill": {
     "duration": 0.016317,
     "end_time": "2022-10-03T03:54:19.805975",
     "exception": false,
     "start_time": "2022-10-03T03:54:19.789658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, tf, patch_size):\n",
    "    # read image -> convert to RGB -> torch Tensor\n",
    "    rgb_img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    img = tf(rgb_img)\n",
    "    _, image_height, image_width = img.shape\n",
    "    \n",
    "    # make the image divisible by the patch size\n",
    "    w, h = image_width - image_width % patch_size, image_height - image_height % patch_size\n",
    "    img = img[:, :h, :w].unsqueeze(0)\n",
    "    \n",
    "    w_featmap = img.shape[-1] // patch_size\n",
    "    h_featmap = img.shape[-2] // patch_size\n",
    "    return rgb_img, img, w_featmap, h_featmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ed35e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:19.819563Z",
     "iopub.status.busy": "2022-10-03T03:54:19.819276Z",
     "iopub.status.idle": "2022-10-03T03:54:19.826200Z",
     "shell.execute_reply": "2022-10-03T03:54:19.825147Z"
    },
    "papermill": {
     "duration": 0.016299,
     "end_time": "2022-10-03T03:54:19.828530",
     "exception": false,
     "start_time": "2022-10-03T03:54:19.812231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_attentions(img, w_featmap, h_featmap, patch_size, mode = 'bilinear'):\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n",
    "    nh = attentions.shape[1]\n",
    "    \n",
    "    # we keep only the output patch attention\n",
    "    # reshape to image size\n",
    "    attentions = attentions[0, :, :, 0].reshape(nh, h_featmap, w_featmap)\n",
    "    attentions = F.interpolate(attentions.unsqueeze(0), scale_factor=patch_size, mode=mode)[0].detach().cpu().numpy()\n",
    "    return attentions\n",
    "\n",
    "\n",
    "def get_attention_masks(image_path, model, transform, patch_size, mode = 'bilinear'):\n",
    "    rgb_img, img, w_featmap, h_featmap = preprocess_image(image_path, transform, patch_size)\n",
    "    attentions = calculate_attentions(img, w_featmap, h_featmap, patch_size, mode = mode)\n",
    "    return rgb_img, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0ec7dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:19.842698Z",
     "iopub.status.busy": "2022-10-03T03:54:19.842056Z",
     "iopub.status.idle": "2022-10-03T03:54:19.850117Z",
     "shell.execute_reply": "2022-10-03T03:54:19.849234Z"
    },
    "papermill": {
     "duration": 0.017269,
     "end_time": "2022-10-03T03:54:19.852119",
     "exception": false,
     "start_time": "2022-10-03T03:54:19.834850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_segformer_stage_attentions(img, num_stages, mode = 'bilinear'):\n",
    "    stages_data = model.get_attention_outputs(img.to(device))\n",
    "    stage_attn_output = []\n",
    "\n",
    "    for i, data in enumerate(stages_data[0:num_stages]):\n",
    "        stage_attn = data['attn']\n",
    "        stage_nh = stage_attn.shape[1]\n",
    "\n",
    "        # we keep only the output patch attention\n",
    "        # reshape to image size\n",
    "        stage_attn = stage_attn[0, :, :, 0]\n",
    "        stage_h, stage_w = int(targetHeight / stage_scale[i]), int(targetWidth / stage_scale[i])\n",
    "        stage_attn = stage_attn.reshape(stage_nh, stage_h, stage_w)\n",
    "        stage_attn = F.interpolate(stage_attn.unsqueeze(0), size=(targetHeight, targetWidth), mode=mode)[0].detach().cpu().numpy()\n",
    "        stage_attn_output.append(stage_attn)\n",
    "    \n",
    "    stage_attn_output = np.concatenate(stage_attn_output, axis=0)    \n",
    "    return stage_attn_output\n",
    "\n",
    "\n",
    "def get_stage_attention_masks(image_path, model, transform, patch_size, num_stages, mode = 'bilinear'):\n",
    "    rgb_img, img, w_featmap, h_featmap = preprocess_image(image_path, transform, patch_size)\n",
    "    attentions = calculate_segformer_stage_attentions(img, num_stages = num_stages, mode = mode)\n",
    "    return rgb_img, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237c4557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:19.866518Z",
     "iopub.status.busy": "2022-10-03T03:54:19.865640Z",
     "iopub.status.idle": "2022-10-03T03:54:19.927246Z",
     "shell.execute_reply": "2022-10-03T03:54:19.926225Z"
    },
    "papermill": {
     "duration": 0.071135,
     "end_time": "2022-10-03T03:54:19.929663",
     "exception": false,
     "start_time": "2022-10-03T03:54:19.858528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "targetWidth = 1024\n",
    "targetHeight = 512\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "NUM_CLASSES = 19\n",
    "MODEL_NAME = f'segformer_mit_b3_last_stage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b768d0be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:19.944240Z",
     "iopub.status.busy": "2022-10-03T03:54:19.943594Z",
     "iopub.status.idle": "2022-10-03T03:54:27.839206Z",
     "shell.execute_reply": "2022-10-03T03:54:27.838231Z"
    },
    "papermill": {
     "duration": 7.904823,
     "end_time": "2022-10-03T03:54:27.841241",
     "exception": false,
     "start_time": "2022-10-03T03:54:19.936418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = segformer_mit_b3(in_channels=3, num_classes=NUM_CLASSES).to(device)\n",
    "model.eval();\n",
    "checkpoint = torch.load('../input/image-segmentation/segformer_mit_b3_cs_pretrain_19CLS_512_1024_CE_loss.pt')\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6c94af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:27.856689Z",
     "iopub.status.busy": "2022-10-03T03:54:27.855228Z",
     "iopub.status.idle": "2022-10-03T03:54:27.862441Z",
     "shell.execute_reply": "2022-10-03T03:54:27.861619Z"
    },
    "papermill": {
     "duration": 0.016744,
     "end_time": "2022-10-03T03:54:27.864508",
     "exception": false,
     "start_time": "2022-10-03T03:54:27.847764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = '.'\n",
    "patch_size = 32\n",
    "stage_scale = [4, 8, 16, 32]\n",
    "stage_heads = [1, 2, 5, 8]\n",
    "titles = []\n",
    "for stage_index, stage_nh in enumerate(stage_heads):\n",
    "    titles.extend([f\"STAGE_{stage_index+1}_HEAD_{x+1}\" for x in range(stage_nh)])\n",
    "\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41867505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:27.879721Z",
     "iopub.status.busy": "2022-10-03T03:54:27.878360Z",
     "iopub.status.idle": "2022-10-03T03:54:28.210689Z",
     "shell.execute_reply": "2022-10-03T03:54:28.209703Z"
    },
    "papermill": {
     "duration": 0.34237,
     "end_time": "2022-10-03T03:54:28.213416",
     "exception": false,
     "start_time": "2022-10-03T03:54:27.871046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dir = '../input/cityscapes-512x1024/demoVideo/stuttgart_00'\n",
    "image_list = sorted(os.listdir(input_dir))\n",
    "images_path = [os.path.join(input_dir, x) for x in image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18475710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:28.228922Z",
     "iopub.status.busy": "2022-10-03T03:54:28.228636Z",
     "iopub.status.idle": "2022-10-03T03:54:28.233115Z",
     "shell.execute_reply": "2022-10-03T03:54:28.232227Z"
    },
    "papermill": {
     "duration": 0.013955,
     "end_time": "2022-10-03T03:54:28.235238",
     "exception": false,
     "start_time": "2022-10-03T03:54:28.221283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "font = {'family' : 'normal', 'weight' : 'bold', 'size'   : 4}\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams['text.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c4970f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T03:54:28.249043Z",
     "iopub.status.busy": "2022-10-03T03:54:28.248784Z",
     "iopub.status.idle": "2022-10-03T04:12:09.905259Z",
     "shell.execute_reply": "2022-10-03T04:12:09.904223Z"
    },
    "papermill": {
     "duration": 1061.666423,
     "end_time": "2022-10-03T04:12:09.907890",
     "exception": false,
     "start_time": "2022-10-03T03:54:28.241467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [17:40<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "fig, axes = plt.subplots(3,3, figsize=(15.5,8))\n",
    "axes = axes.flatten()\n",
    "fig.tight_layout()\n",
    "\n",
    "for image_path in tqdm(images_path):\n",
    "    image_name = image_path.split(os.sep)[-1].split('.')[0]\n",
    "    \n",
    "    rgb_img, attentions = get_attention_masks(image_path, model, transform, patch_size, mode = 'bilinear')\n",
    "#     rgb_img, attentions = get_stage_attention_masks(image_path, model, transform, patch_size, num_stages=3, mode = 'bilinear')    \n",
    "        \n",
    "#     for i in range(len(axes)):\n",
    "#         axes[i].clear()\n",
    "#         axes[i].imshow(rgb_img)\n",
    "#         axes[i].imshow(attentions[i], cmap='inferno', alpha=0.5)\n",
    "#         axes[i].axis('off')\n",
    "#         axes[i].set_title(titles[i], x= 0.22, y=0.9, va=\"top\")\n",
    "    \n",
    "    \n",
    "###########################################    \n",
    "    for i in range(len(axes)):\n",
    "        axes[i].clear()\n",
    "        if (i < 4):\n",
    "            axes[i].imshow(rgb_img)\n",
    "            axes[i].imshow(attentions[i], cmap='inferno', alpha=0.5)\n",
    "            axes[i].set_title(titles[i+8], x= 0.20, y=0.9, va=\"top\")\n",
    "            \n",
    "        elif(i==4):\n",
    "            axes[i].imshow(np.zeros_like(rgb_img))\n",
    "        else:\n",
    "            axes[i].imshow(rgb_img)\n",
    "            axes[i].imshow(attentions[i-1], cmap='inferno', alpha=0.5)\n",
    "            axes[i].set_title(titles[i-1+8], x= 0.20, y=0.9, va=\"top\")\n",
    "\n",
    "        axes[i].axis('off')\n",
    "\n",
    "###########################################\n",
    "\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    fig.savefig(f'{image_name}_last_stage.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43fe042b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T04:12:09.985680Z",
     "iopub.status.busy": "2022-10-03T04:12:09.985335Z",
     "iopub.status.idle": "2022-10-03T04:12:09.993284Z",
     "shell.execute_reply": "2022-10-03T04:12:09.992380Z"
    },
    "papermill": {
     "duration": 0.048227,
     "end_time": "2022-10-03T04:12:09.995294",
     "exception": false,
     "start_time": "2022-10-03T04:12:09.947067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_images_to_video(images_dir, output_video_path, fps : int = 20):\n",
    "    \n",
    "    input_images = [os.path.join(images_dir, *[x]) for x in sorted(os.listdir(images_dir)) if x.endswith('png')]\n",
    "    \n",
    "    if(len(input_images) > 0):\n",
    "        sample_image = cv2.imread(input_images[0])\n",
    "        height, width, _ = sample_image.shape\n",
    "        \n",
    "        # handles for input output videos\n",
    "        output_handle = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "\n",
    "        # create progress bar\n",
    "        num_frames = int(len(input_images))\n",
    "        pbar = tqdm(total = num_frames, position=0, leave=True)\n",
    "\n",
    "        for i in tqdm(range(num_frames), position=0, leave=True):\n",
    "            frame = cv2.imread(input_images[i])\n",
    "            output_handle.write(frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "        # release the output video handler\n",
    "        output_handle.release()\n",
    "                \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad9cd734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T04:12:10.067695Z",
     "iopub.status.busy": "2022-10-03T04:12:10.067417Z",
     "iopub.status.idle": "2022-10-03T04:12:10.071959Z",
     "shell.execute_reply": "2022-10-03T04:12:10.070933Z"
    },
    "papermill": {
     "duration": 0.04338,
     "end_time": "2022-10-03T04:12:10.074054",
     "exception": false,
     "start_time": "2022-10-03T04:12:10.030674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createDir(dirPath):\n",
    "    if(not os.path.isdir(dirPath)):\n",
    "        os.mkdir(dirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c493c800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T04:12:10.146809Z",
     "iopub.status.busy": "2022-10-03T04:12:10.146552Z",
     "iopub.status.idle": "2022-10-03T04:12:10.152564Z",
     "shell.execute_reply": "2022-10-03T04:12:10.151319Z"
    },
    "papermill": {
     "duration": 0.044705,
     "end_time": "2022-10-03T04:12:10.154703",
     "exception": false,
     "start_time": "2022-10-03T04:12:10.109998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./videos/segformer_mit_b3_last_stage_last_stage_demoVideo.mp4\n"
     ]
    }
   ],
   "source": [
    "video_output_dir = os.path.join(output_dir, *['videos'])\n",
    "createDir(video_output_dir)\n",
    "output_video_path = os.path.join(video_output_dir, *[f\"{MODEL_NAME}_last_stage_demoVideo.mp4\"])\n",
    "print(output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b45273f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-03T04:12:10.226753Z",
     "iopub.status.busy": "2022-10-03T04:12:10.226497Z",
     "iopub.status.idle": "2022-10-03T04:12:44.485792Z",
     "shell.execute_reply": "2022-10-03T04:12:44.484702Z"
    },
    "papermill": {
     "duration": 34.297318,
     "end_time": "2022-10-03T04:12:44.487733",
     "exception": false,
     "start_time": "2022-10-03T04:12:10.190415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 599/599 [00:34<00:00, 17.54it/s]\n",
      "100%|██████████| 599/599 [00:34<00:00, 17.53it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_images_to_video('./', output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a0b8d",
   "metadata": {
    "papermill": {
     "duration": 0.065514,
     "end_time": "2022-10-03T04:12:44.619977",
     "exception": false,
     "start_time": "2022-10-03T04:12:44.554463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1156.80005,
   "end_time": "2022-10-03T04:12:46.992502",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-03T03:53:30.192452",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
