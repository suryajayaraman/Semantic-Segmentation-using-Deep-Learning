{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4b6203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\surya\\miniconda37\\envs\\imgseg\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "\n",
    "# DL imports\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn, einsum\n",
    "from functools import partial\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381958b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3151d120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.011872336272725e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**-4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "869215fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0xfce3e10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.nn.Linear(2, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=100)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "lrs = []\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "#     print(\"Factor = \",0.1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n",
    "    scheduler.step()\n",
    "\n",
    "plt.plot(lrs)\n",
    "plt.scatter(range(len(lrs)), lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320ee28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_tuple(val, depth):\n",
    "    return val if isinstance(val, tuple) else (val,) * depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02836b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DsConv2d(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding, stride = 1, bias = True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias = bias),\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0acf2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsConvInput = torch.Tensor(2,3,180,320)\n",
    "# print(\"dsConvInput.shape = \", dsConvInput.shape)\n",
    "# model = DsConv2d(dim_in=3, dim_out=12, kernel_size=3, padding=1)\n",
    "# dsConvOutput = model(dsConvInput)\n",
    "# print(\"dsConvOutput.shape = \", dsConvOutput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6074542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim = 1, unbiased = False, keepdim = True).sqrt()\n",
    "#         print(\"std shape = \", std.shape)\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "#         print(\"mean shape = \", mean.shape)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566defe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layerNormInput = torch.Tensor(10,3,180,320)\n",
    "# print(\"layerNormInput.shape = \" , layerNormInput.shape)\n",
    "# model = LayerNorm(dim=3)\n",
    "# layerNormOutput = model(layerNormInput)\n",
    "# print(\"layerNormOutput.shape = \" , layerNormOutput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b1ebf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171f7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixFeedForward(nn.Module):\n",
    "    def __init__(self, *, dim, expansion_factor):\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * expansion_factor\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, 1),\n",
    "            DsConv2d(hidden_dim, hidden_dim, 3, padding = 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_dim, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e237f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixFeedForwardInput = torch.Tensor(2,3,180,320)\n",
    "# print(\"mixFeedForwardInput.shape = \" , mixFeedForwardInput.shape)\n",
    "# model = MixFeedForward(dim=3, expansion_factor=4)\n",
    "# mixFeedForwardOutput = model(mixFeedForwardInput)\n",
    "# print(\"mixFeedForwardOutput.shape = \" , mixFeedForwardOutput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1dca3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(self, *, dim, heads, reduction_ratio):\n",
    "        super().__init__()\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, dim, 1, bias = False)\n",
    "        self.to_kv = nn.Conv2d(dim, dim * 2, reduction_ratio, stride = reduction_ratio, bias = False)\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        heads = self.heads\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = 1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = heads), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) (x y) c -> b (h c) x y', h = heads, x = h, y = w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e422a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiT(nn.Module):\n",
    "    def __init__(self, *, channels, dims, heads, ff_expansion, reduction_ratio, num_layers):\n",
    "        super().__init__()\n",
    "        stage_kernel_stride_pad = ((7, 4, 3), (3, 2, 1), (3, 2, 1), (3, 2, 1))\n",
    "\n",
    "        dims = (channels, *dims)\n",
    "        dim_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "        self.stages = nn.ModuleList([])\n",
    "\n",
    "        for (dim_in, dim_out), (kernel, stride, padding), num_layers, ff_expansion, heads, reduction_ratio in zip(dim_pairs, stage_kernel_stride_pad, num_layers, ff_expansion, heads, reduction_ratio):\n",
    "            get_overlap_patches = nn.Unfold(kernel, stride = stride, padding = padding)\n",
    "            overlap_patch_embed = nn.Conv2d(dim_in * kernel ** 2, dim_out, 1)\n",
    "\n",
    "            layers = nn.ModuleList([])\n",
    "\n",
    "            for _ in range(num_layers):\n",
    "                layers.append(nn.ModuleList([\n",
    "                    PreNorm(dim_out, EfficientSelfAttention(dim = dim_out, heads = heads, reduction_ratio = reduction_ratio)),\n",
    "                    PreNorm(dim_out, MixFeedForward(dim = dim_out, expansion_factor = ff_expansion)),\n",
    "                ]))\n",
    "\n",
    "            self.stages.append(nn.ModuleList([\n",
    "                get_overlap_patches,\n",
    "                overlap_patch_embed,\n",
    "                layers\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, return_layer_outputs = False):\n",
    "        h, w = x.shape[-2:]\n",
    "\n",
    "        layer_outputs = []\n",
    "        for (get_overlap_patches, overlap_embed, layers) in self.stages:\n",
    "            x = get_overlap_patches(x)\n",
    "\n",
    "            num_patches = x.shape[-1]\n",
    "            ratio = int(np.sqrt((h * w) / num_patches))\n",
    "            x = rearrange(x, 'b c (h w) -> b c h w', h = h // ratio)\n",
    "\n",
    "            x = overlap_embed(x)\n",
    "            for (attn, ff) in layers:\n",
    "                x = attn(x) + x\n",
    "                x = ff(x) + x\n",
    "\n",
    "            layer_outputs.append(x)\n",
    "\n",
    "        ret = x if not return_layer_outputs else layer_outputs\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30884426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, *, dims = (32, 64, 160, 256), heads = (1, 2, 5, 8), ff_expansion = (8, 8, 4, 4),\n",
    "         reduction_ratio = (8, 4, 2, 1), num_layers = 2, channels = 3, decoder_dim = 256, num_classes = 4):\n",
    "        super().__init__()\n",
    "        dims, heads, ff_expansion, reduction_ratio, num_layers = map(partial(cast_tuple, depth = 4), (dims, heads, ff_expansion, reduction_ratio, num_layers))\n",
    "        assert all([*map(lambda t: len(t) == 4, (dims, heads, ff_expansion, reduction_ratio, num_layers))]), 'only four stages are allowed, all keyword arguments must be either a single value or a tuple of 4 values'\n",
    "\n",
    "        self.mit = MiT(\n",
    "            channels = channels,\n",
    "            dims = dims,\n",
    "            heads = heads,\n",
    "            ff_expansion = ff_expansion,\n",
    "            reduction_ratio = reduction_ratio,\n",
    "            num_layers = num_layers\n",
    "        )\n",
    "\n",
    "        self.to_fused = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv2d(dim, decoder_dim, 1),\n",
    "            nn.Upsample(scale_factor = 2 ** i)\n",
    "        ) for i, dim in enumerate(dims)])\n",
    "\n",
    "        self.to_segmentation = nn.Sequential(\n",
    "            nn.Conv2d(4 * decoder_dim, decoder_dim, 1),\n",
    "            nn.Conv2d(decoder_dim, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h,w = x.size()[2:]\n",
    "        if(((h % 32 ==0) and (w % 32 ==0)) == False):\n",
    "            temp_h = round(h / 32) * 32\n",
    "            temp_w = round(w / 32) * 32\n",
    "            x = F.interpolate(x, (temp_h, temp_w), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        layer_outputs = self.mit(x, return_layer_outputs = True)\n",
    "        fused = [to_fused(output) for output, to_fused in zip(layer_outputs, self.to_fused)]\n",
    "        fused = torch.cat(fused, dim = 1)\n",
    "        decoder_output = self.to_segmentation(fused)\n",
    "        return F.interpolate(decoder_output, (h,w), mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b755d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 360, 640])\n"
     ]
    }
   ],
   "source": [
    "model = Segformer(\n",
    "    dims = (32, 64, 160, 256),      # dimensions of each stage\n",
    "    heads = (1, 2, 4, 8),           # heads of each stage\n",
    "    ff_expansion = (8, 8, 4, 4),    # feedforward expansion factor of each stage\n",
    "    reduction_ratio = (8, 4, 2, 1), # reduction ratio of each stage for efficient attention\n",
    "    num_layers = 2,                 # num layers of each stage\n",
    "    decoder_dim = 256,              # decoder dimension\n",
    "    num_classes = 3                 # number of segmentation classes\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 3, 360, 640)\n",
    "pred = model(x)\n",
    "print(pred.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PSPNet_starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(imgseg)",
   "language": "python",
   "name": "imgseg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
